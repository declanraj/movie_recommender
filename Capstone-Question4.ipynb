{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65b545d5",
   "metadata": {},
   "source": [
    "### Question 4 small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5383f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, count, row_number\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import IntegerType, FloatType, StructType, StructField\n",
    "from pyspark.mllib.evaluation import RankingMetrics\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71e171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/05/11 21:54:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"question4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4b36475",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "train_small = spark.read.parquet(\"rec-small/train.parquet\", header=True, schema=ratings_schema)\n",
    "val_small = spark.read.parquet(\"rec-small/val.parquet\", header=True, schema=ratings_schema)\n",
    "test_small = spark.read.parquet(\"rec-small/test.parquet\", header=True, schema=ratings_schema)\n",
    "# Row(userId=503, movieId=68157, rating=4.5, timestamp=1335219485)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e9b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_movies = train_small.groupBy(\"movieId\").agg(\n",
    "                        avg(\"rating\").alias(\"avg_rating\"), \n",
    "                        count(\"rating\").alias(\"num_ratings\"))\n",
    "\n",
    "damping_factor = 1000\n",
    "popular_movies = popular_movies.withColumn(\"damped_popularity\",\n",
    "                                        (col(\"avg_rating\")*col(\"num_ratings\"))/(col(\"num_ratings\") + damping_factor)\n",
    "                                        )\n",
    "popular_movies = popular_movies.orderBy(\n",
    "                            col(\"damped_popularity\").desc())\n",
    "\n",
    "                                                                                \n",
    "# Row(movieId=318, avg_rating=4.448339483394834, num_ratings=271, damped_popularity=0.9484657749803305)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "835c742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small_joined = test_small.join(popular_movies, \"movieId\", \"left\")\n",
    "# Row(movieId=68157, userId=503, rating=4.5, timestamp=1335219485, avg_rating=4.212328767123288, num_ratings=73, damped_popularity=0.28657968313140725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56f99c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('userId').orderBy(F.desc('damped_popularity'))\n",
    "ranked_test_small_joined = test_small_joined.withColumn('rank', F.rank().over(window_spec))\n",
    "# Row(movieId=2959, userId=65, rating=4.5, timestamp=1494767045, avg_rating=4.3138297872340425, num_ratings=188, damped_popularity=0.6826599326599326, rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e5add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_per_user = ranked_test_small_joined.filter(col(\"rank\") <= 100)\n",
    "top_100_per_user = top_100_per_user.drop(col(\"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ecc41b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "top_100_per_user = top_100_per_user.rdd.map(\n",
    "                        lambda row: (row[\"userId\"], row[\"movieId\"])).groupByKey().mapValues(list)\n",
    "# (1, [356, 296, 2571, 260, 2959])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3489b5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings_per_user = test_small.groupBy(\"userId\").agg(F.mean(\"rating\").alias(\"mean_rating\"))                                            \n",
    "# Row(userId=148, mean_rating=3.7395833333333335)\n",
    "\n",
    "movies_with_mean = test_small.join(mean_ratings_per_user, \"userId\", \"inner\")\n",
    "# Row(userId=1, movieId=1, rating=4.0, timestamp=964982703, mean_rating=4.366379310344827)\n",
    "\n",
    "movies_above_mean = movies_with_mean.filter(col(\"rating\") > col(\"mean_rating\"))\n",
    "# Row(userId=1, movieId=1, rating=4.0, timestamp=964982703, mean_rating=4.366379310344827)\n",
    "\n",
    "\n",
    "\n",
    "movies_above_mean_rdd = movies_above_mean.rdd.map(\n",
    "                        lambda row: (row[\"userId\"], row[\"movieId\"])).groupByKey().mapValues(list)\n",
    "\n",
    "# (21,[2376, 4545, 54286, 7570])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b49300b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds_and_labels = top_100_per_user.join(movies_above_mean_rdd).map(lambda row: (row[1][0], row[1][1])).collect()\n",
    "preds_and_labels_par = spark.sparkContext.parallelize(preds_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c4d9a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score on full test dataset (users with > 10 movies watched):  0.6481317429645803\n",
      "NDCG at 5 on full test dataset (users with > 10 movies watched):  0.7660117132601572\n",
      "NDCG at 100 on full test dataset (users with > 10 movies watched):  0.8459597294943321\n",
      "Recall at 5 on full test dataset (users with > 10 movies watched):  0.22462918722627528\n",
      "Recall at 100 on full test dataset (users with > 10 movies watched):  0.9059719034064517\n"
     ]
    }
   ],
   "source": [
    "metrics = RankingMetrics(preds_and_labels_par)\n",
    "\n",
    "MAP_score = metrics.meanAveragePrecision\n",
    "ndcgAt5 = metrics.ndcgAt(5)\n",
    "ndcgAt100 = metrics.ndcgAt(100)\n",
    "recallAt5 = metrics.recallAt(5)\n",
    "recallAt100 = metrics.recallAt(100)\n",
    "\n",
    "print(\"MAP Score on full test dataset (users with > 10 movies watched): \", MAP_score)\n",
    "print(\"NDCG at 5 on full test dataset (users with > 10 movies watched): \", ndcgAt5)\n",
    "print(\"NDCG at 100 on full test dataset (users with > 10 movies watched): \", ndcgAt100)\n",
    "print(\"Recall at 5 on full test dataset (users with > 10 movies watched): \", recallAt5)\n",
    "print(\"Recall at 100 on full test dataset (users with > 10 movies watched): \", recallAt100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30156a39",
   "metadata": {},
   "source": [
    "### Question 4 All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e93cec34",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_schema = StructType([\n",
    "    StructField(\"userId\", IntegerType(), True),\n",
    "    StructField(\"movieId\", IntegerType(), True),\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"timestamp\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "train = spark.read.parquet(\"rec/train.parquet\", header=True, schema=ratings_schema)\n",
    "val = spark.read.parquet(\"rec/val.parquet\", header=True, schema=ratings_schema)\n",
    "test = spark.read.parquet(\"rec/test.parquet\", header=True, schema=ratings_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b103858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_movies = train.groupBy(\"movieId\").agg(\n",
    "                        avg(\"rating\").alias(\"avg_rating\"), \n",
    "                        count(\"rating\").alias(\"num_ratings\"))\n",
    "\n",
    "damping_factor = 1000\n",
    "popular_movies = popular_movies.withColumn(\"damped_popularity\",\n",
    "                                        (col(\"avg_rating\")*col(\"num_ratings\"))/(col(\"num_ratings\") + damping_factor)\n",
    "                                        )\n",
    "popular_movies = popular_movies.orderBy(\n",
    "                            col(\"damped_popularity\").desc())\n",
    "\n",
    "# Row(movieId=318, avg_rating=4.4110477257678395, num_ratings=98165, damped_popularity=4.366565824635708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a69ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_joined = test.join(popular_movies, \"movieId\", \"left\")\n",
    "# Row(movieId=68157, userId=503, rating=4.5, timestamp=1335219485, avg_rating=4.212328767123288, num_ratings=73, damped_popularity=0.28657968313140725)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ae4bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_spec = Window.partitionBy('userId').orderBy(F.desc('damped_popularity'))\n",
    "ranked_test_joined = test_joined.withColumn('rank', F.rank().over(window_spec))\n",
    "# Row(movieId=2959, userId=65, rating=4.5, timestamp=1494767045, avg_rating=4.3138297872340425, num_ratings=188, damped_popularity=0.6826599326599326, rank=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53aadbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_per_user = ranked_test_joined.filter(col(\"rank\") <= 100)\n",
    "top_100_per_user = top_100_per_user.drop(col(\"rank\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc41a294",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_100_per_user = top_100_per_user.rdd.map(\n",
    "                        lambda row: (row[\"userId\"], row[\"movieId\"])).groupByKey().mapValues(list)\n",
    "# (1, [356, 296, 2571, 260, 2959])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3d0b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ratings_per_user = test.groupBy(\"userId\").agg(F.mean(\"rating\").alias(\"mean_rating\"))                                            \n",
    "# Row(userId=148, mean_rating=3.7395833333333335)\n",
    "\n",
    "movies_with_mean = test.join(mean_ratings_per_user, \"userId\", \"inner\")\n",
    "# Row(userId=1, movieId=1, rating=4.0, timestamp=964982703, mean_rating=4.366379310344827)\n",
    "\n",
    "movies_above_mean = movies_with_mean.filter(col(\"rating\") > col(\"mean_rating\"))\n",
    "# Row(userId=1, movieId=1, rating=4.0, timestamp=964982703, mean_rating=4.366379310344827)\n",
    "\n",
    "\n",
    "movies_above_mean_rdd = movies_above_mean.rdd.map(\n",
    "                        lambda row: (row[\"userId\"], row[\"movieId\"])).groupByKey().mapValues(list)\n",
    "\n",
    "# (21,[2376, 4545, 54286, 7570])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2eedd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "preds_and_labels = top_100_per_user.join(movies_above_mean_rdd).map(lambda row: (row[1][0], row[1][1])).collect()\n",
    "preds_and_labels_par = spark.sparkContext.parallelize(preds_and_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bd2c9778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/11 21:54:55 WARN TaskSetManager: Stage 29 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/11 21:54:55 WARN TaskSetManager: Stage 30 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/11 21:54:55 WARN TaskSetManager: Stage 31 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/11 21:54:56 WARN TaskSetManager: Stage 32 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/11 21:54:56 WARN TaskSetManager: Stage 33 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/11 21:54:57 WARN TaskSetManager: Stage 34 contains a task of very large size (1439 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score on full test dataset (users with > 10 movies watched):  0.683826696502222\n",
      "NDCG at 5 on full test dataset (users with > 10 movies watched):  0.7439731984274025\n",
      "NDCG at 100 on full test dataset (users with > 10 movies watched):  0.8485859585672962\n",
      "Recall at 5 on full test dataset (users with > 10 movies watched):  0.38240747502257816\n",
      "Recall at 100 on full test dataset (users with > 10 movies watched):  0.9456506802535892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = RankingMetrics(preds_and_labels_par)\n",
    "\n",
    "MAP_score = metrics.meanAveragePrecision\n",
    "ndcgAt5 = metrics.ndcgAt(5)\n",
    "ndcgAt100 = metrics.ndcgAt(100)\n",
    "recallAt5 = metrics.recallAt(5)\n",
    "recallAt100 = metrics.recallAt(100)\n",
    "\n",
    "print(\"MAP Score on full test dataset (users with > 10 movies watched): \", MAP_score)\n",
    "print(\"NDCG at 5 on full test dataset (users with > 10 movies watched): \", ndcgAt5)\n",
    "print(\"NDCG at 100 on full test dataset (users with > 10 movies watched): \", ndcgAt100)\n",
    "print(\"Recall at 5 on full test dataset (users with > 10 movies watched): \", recallAt5)\n",
    "print(\"Recall at 100 on full test dataset (users with > 10 movies watched): \", recallAt100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
