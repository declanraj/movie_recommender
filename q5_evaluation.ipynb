{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa2f7c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, when, desc, rank, mean\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.mllib.evaluation import RankingMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa46d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(spark, userID):\n",
    "\n",
    "    train = spark.read.parquet('/scratch/sjm643/sp24_bigd/rec/train.parquet')\n",
    "    val = spark.read.parquet('/scratch/sjm643/sp24_bigd/rec/val.parquet')\n",
    "    test = spark.read.parquet('/scratch/sjm643/sp24_bigd/rec/test.parquet')\n",
    "    \n",
    "    train_full = train.union(val)\n",
    "    ## from train-validation\n",
    "    #best hyperparameters: reg=0.05, rank=150\n",
    "    best_rank = 150\n",
    "    best_reg = 0.05\n",
    "    als = ALS(rank=best_rank, maxIter=10, regParam=best_reg,\n",
    "                      userCol='userId', itemCol='movieId', ratingCol='rating', coldStartStrategy='drop')\n",
    "\n",
    "    model = als.fit(train_full)\n",
    "    predictions = model.transform(test)\n",
    "            \n",
    "    window_spec = Window.partitionBy('userId').orderBy(desc('prediction'))\n",
    "    ranked_pred = predictions.withColumn('rank', rank().over(window_spec))\n",
    "    top_100_per_user = ranked_pred.filter(col('rank') <= 100)\n",
    "    top_100_per_user = top_100_per_user.drop(col('rank'))\n",
    "\n",
    "    top_100_per_user_rdd = top_100_per_user.rdd.map(\n",
    "        lambda row: (row['userId'], row['movieId'])).groupByKey().mapValues(list)\n",
    "\n",
    "    mean_ratings_per_user = test.groupBy('userId').agg(mean(\"rating\").alias(\"mean_rating\"))                                            \n",
    "\n",
    "    movies_with_mean = test.join(mean_ratings_per_user, 'userId', 'inner')\n",
    "    movies_above_mean = movies_with_mean.filter(col('rating') > col('mean_rating'))\n",
    "\n",
    "    movies_above_mean_rdd = movies_above_mean.rdd.map(\n",
    "        lambda row: (row['userId'], row['movieId'])).groupByKey().mapValues(list)\n",
    "\n",
    "    preds_and_labels = top_100_per_user_rdd.join(movies_above_mean_rdd).map(lambda row: (row[1][0], row[1][1])).collect()\n",
    "    preds_and_labels_par = spark.sparkContext.parallelize(preds_and_labels)   \n",
    "\n",
    "    metrics = RankingMetrics(preds_and_labels_par)\n",
    "\n",
    "    MAP = metrics.meanAveragePrecision\n",
    "    ndcgAt5 = metrics.ndcgAt(5)\n",
    "    ndcgAt100 = metrics.ndcgAt(100)\n",
    "    recallAt5 = metrics.recallAt(5)\n",
    "    recallAt100 = metrics.recallAt(100)\n",
    "\n",
    "    print(\"MAP Score on full test dataset (users with > 10 movies watched): \", MAP)\n",
    "    print(\"NDCG at 5 on full test dataset (users with > 10 movies watched): \", ndcgAt5)\n",
    "    print(\"NDCG at 100 on full test dataset (users with > 10 movies watched): \", ndcgAt100)\n",
    "    print(\"Recall at 5 on full test dataset (users with > 10 movies watched): \", recallAt5)\n",
    "    print(\"Recall at 100 on full test dataset (users with > 10 movies watched): \", recallAt100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e9750d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/ext3/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "24/05/12 01:08:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/12 01:08:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "24/05/12 01:08:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "24/05/12 01:08:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "24/05/12 01:08:27 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP Score on full test dataset (users with > 10 movies watched):  0.7507122400143744\n",
      "NDCG at 5 on full test dataset (users with > 10 movies watched):  0.8289041684546175\n",
      "NDCG at 100 on full test dataset (users with > 10 movies watched):  0.8908813224744427\n",
      "Recall at 5 on full test dataset (users with > 10 movies watched):  0.4063115459577412\n",
      "Recall at 100 on full test dataset (users with > 10 movies watched):  0.9561633671706878\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Application\") \\\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.broadcastTimeout\", \"7200\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseG1GC\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "    userID = os.environ['USER']\n",
    "\n",
    "    main(spark, userID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
